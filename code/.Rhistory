# Train the SVM model
svm_model <- svm(formula, data = train_data, kernel = kernel, cost = 1, probability = TRUE)
# Predict on test data
preds_prob <- predict(svm_model, test_data, type = "probabilities")
# Confusion Matrix
cm <- table(test_data$poor_20, preds_prob, dnn = c("Actual", "Predicted"))
# Calculate metrics
accuracy <- sum(diag(cm)) / sum(cm)
precision <- cm[2, 2] / sum(cm[, 2])
sensitivity <- cm[2, 2] / sum(cm[2, ])
f1_score <- 2 * ((precision * sensitivity) / (precision + sensitivity))
roc_obj <- roc(test_data$poor_20, as.numeric(preds_prob))
auc <- auc(roc_obj)
# Add metrics to the table
metrics_table <- rbind(metrics_table, data.frame(Kernel = kernel, Accuracy = accuracy,
Precision = precision, sensitivity = sensitivity,
F1_Score = f1_score, AUC = auc))
}
# Print the metrics table
print(metrics_table)
xtable(cm)
class(data$poor_20)
#----
# A Poor Means Test Replication Study
#Classification using SVM and XGBoost
#Author : Sol Yates
## TO DO - DOUBLE CHECK confusion matrices, implement SVR + CART?
rm(list=ls())
setwd("/home/oddish3/Documents/R_folder/UG/PMT/Scripts")
# Required libraries
library(haven)
library(e1071)
library(magrittr)
library(dplyr)
library(caTools)
library(ggplot2)
library(xgboost)
library(caret)
library(pROC)
library(dplyr)
#library(smotefamily)
library(xtable)
library(rBayesianOptimization)
data <- read_dta("../dta_files/data_over_sampled.dta")
# Train/test split 80% train, 20% test
data$poor_20 <- as.factor(data$poor_20)
train_index <- createDataPartition(data$poor_20, p = 0.8, list = FALSE, times = 1)
#----
# A Poor Means Test Replication Study
#
#Author : Sol Yates
rm(list=ls())
setwd("/home/oddish3/Documents/R_folder/UG/PMT/Scripts")
# Required libraries
library(haven)
library(magrittr)
library(dplyr)
library(ggplot2)
library(ROSE)
library(smotefamily)
data <- read_dta("../dta_files/PMT_CLEAN_SY2.dta")
data <- data %>% select(c("poor_20", "toilet_pit", "wall_finish",
"fuel_elecgas", "fuel_charcoal","urban", "female_head",
"edu_head_primary", "edu_head_secondary", "div_sep_head", "widow_head",
"work_paid_head", "work_selfemp_nonf_head"))#, "muslim", "christian"))
data$poor_20 <- as.factor(as.character(data$poor_20))
## class imbalance -----
table(data$poor_20)
original_data <- data
# over sampling
set.seed(123)  # For reproducibility
over_sampled_data <- ovun.sample(poor_20 ~ ., data = original_data, method = "over", N = max(table(original_data$poor_20)) * 2)$data
table(over_sampled_data$poor_20)
# under sampling
under_sampled_data <- ovun.sample(poor_20 ~ ., data = original_data, method = "under", N = min(table(original_data$poor_20)) * 2)$data
table(under_sampled_data$poor_20)
write_dta(over_sampled_data, "../dta_files/data_over_sampled.dta")
View(over_sampled_data)
write_dta(under_sampled_data, "../dta_files/data_under_sampled.dta")
# Separate predictors and target
X <- data[, setdiff(names(data), "poor_20")]
target <- data$poor_20
# Apply SMOTE for oversampling to balance the classes
genData <- SMOTE(X = X, target = target, K = 5, dup_size = 1)
# Use the 'data' component from genData which contains the balanced dataset
balanced_df <- genData$data
balanced_df$class <- as.factor(balanced_df$class)
write_dta(balanced_df, "../dta_files/data_smote.dta")
#tables of each class and sampling technique
table(original_data$poor_20)
table(over_sampled_data$poor_20)
table(under_sampled_data$poor_20)
table(balanced_df$class)
## PCA ---------------
class(data$poor_20)
# Remove the target variable before performing PCA
data_pca <- data %>% select(-poor_20)
# Perform PCA - remember to scale the data if needed
pca_result <- prcomp(data_pca, center = TRUE, scale. = TRUE)
# View the summary of PCA results
summary(pca_result)
# Plot the cumulative proportion of variance explained
plot(pca_result$sdev^2 / sum(pca_result$sdev^2), type = 'b', xlab = "Principal Component",
ylab = "Proportion of Variance Explained", ylim = c(0, 1), main = "Scree Plot")
# Cumulative sum of the variance explained
cumulative_variance <- cumsum(pca_result$sdev^2 / sum(pca_result$sdev^2))
plot(cumulative_variance, xlab = "Number of Principal Components", ylab = "Cumulative Proportion of Variance Explained",
type = 'b', main = "Cumulative Scree Plot")
abline(h = 0.8, col = "red", lty = 2)  # Line at 80% variance explained for reference
# Extracting the first 8 principal components
pc_scores <- pca_result$x[, 1:8]
# Creating a new dataframe with PC scores
pc_data <- as.data.frame(pc_scores)
# Adding the target variable back to the dataframe
pc_data$poor_20 <- data$poor_20
write_dta(pc_data, "../dta_files/pc_data.dta")
data <- read_dta("../dta_files/data_over_sampled.dta")
#----
# A Poor Means Test Replication Study
#Classification using SVM and XGBoost
#Author : Sol Yates
## TO DO - DOUBLE CHECK confusion matrices, implement SVR + CART?
rm(list=ls())
setwd("/home/oddish3/Documents/R_folder/UG/PMT/Scripts")
# Required libraries
library(haven)
library(e1071)
library(magrittr)
library(dplyr)
library(caTools)
library(ggplot2)
library(xgboost)
library(caret)
library(pROC)
library(dplyr)
#library(smotefamily)
library(xtable)
library(rBayesianOptimization)
data <- read_dta("../dta_files/data_over_sampled.dta")
# Train/test split 80% train, 20% test
data$poor_20 <- as.factor(data$poor_20)
data <- read_dta("../dta_files/data_over_sampled.dta")
# Train/test split 80% train, 20% test
data$poor_20
#----
# A Poor Means Test Replication Study
#
#Author : Sol Yates
rm(list=ls())
setwd("/home/oddish3/Documents/R_folder/UG/PMT/Scripts")
# Required libraries
library(haven)
library(magrittr)
library(dplyr)
library(ggplot2)
library(ROSE)
library(smotefamily)
data <- read_dta("../dta_files/PMT_CLEAN_SY2.dta")
data <- data %>% select(c("poor_20", "toilet_pit", "wall_finish",
"fuel_elecgas", "fuel_charcoal","urban", "female_head",
"edu_head_primary", "edu_head_secondary", "div_sep_head", "widow_head",
"work_paid_head", "work_selfemp_nonf_head"))#, "muslim", "christian"))
data$poor_20 <- as.factor(as.character(data$poor_20))
# Fit the linear model
lm_model <- lm(poor_20 ~ toilet_pit + wall_finish + fuel_elecgas + fuel_charcoal + urban +
female_head + edu_head_primary + edu_head_secondary + div_sep_head +
widow_head + work_paid_head + work_selfemp_nonf_head,
data = data)
# Predicted values
predictions <- predict(lm_model, data)
plot_data <- data.frame(Actual = data$poor_20, Predicted = predictions)
# Plot using ggplot
ggplot(plot_data, aes(x = Predicted, y = Actual)) +
geom_point(colour = "blue") +  # Add points
geom_smooth(method = "lm", se = FALSE, color = "green") +  # Add a regression line
geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +  # y=x line
labs(title = "Predicted vs. Actual Values", x = "Predicted Values", y = "Actual Values") +
theme_minimal()
dev.off()
## class imbalance -----
table(data$poor_20)
original_data <- data
# over sampling
set.seed(123)  # For reproducibility
over_sampled_data <- ovun.sample(poor_20 ~ ., data = original_data, method = "over", N = max(table(original_data$poor_20)) * 2)$data
table(over_sampled_data$poor_20)
# under sampling
under_sampled_data <- ovun.sample(poor_20 ~ ., data = original_data, method = "under", N = min(table(original_data$poor_20)) * 2)$data
table(under_sampled_data$poor_20)
saveRDS(under_sampled_data, "../dta_files/data_under_sampled.rds")
saveRDS(over_sampled_data, "../dta_files/data_over_sampled.rds")
# Separate predictors and target
X <- data[, setdiff(names(data), "poor_20")]
target <- data$poor_20
# Apply SMOTE for oversampling to balance the classes
genData <- SMOTE(X = X, target = target, K = 5, dup_size = 1)
# Use the 'data' component from genData which contains the balanced dataset
balanced_df <- genData$data
balanced_df$class <- as.factor(balanced_df$class)
saveRDS(balanced_df, "../dta_files/data_smote.rds")
#tables of each class and sampling technique
table(original_data$poor_20)
table(over_sampled_data$poor_20)
table(under_sampled_data$poor_20)
table(balanced_df$class)
## PCA ---------------
class(data$poor_20)
# Remove the target variable before performing PCA
data_pca <- data %>% select(-poor_20)
# Perform PCA - remember to scale the data if needed
pca_result <- prcomp(data_pca, center = TRUE, scale. = TRUE)
# View the summary of PCA results
summary(pca_result)
# Plot the cumulative proportion of variance explained
plot(pca_result$sdev^2 / sum(pca_result$sdev^2), type = 'b', xlab = "Principal Component",
ylab = "Proportion of Variance Explained", ylim = c(0, 1), main = "Scree Plot")
# Cumulative sum of the variance explained
cumulative_variance <- cumsum(pca_result$sdev^2 / sum(pca_result$sdev^2))
plot(cumulative_variance, xlab = "Number of Principal Components", ylab = "Cumulative Proportion of Variance Explained",
type = 'b', main = "Cumulative Scree Plot")
abline(h = 0.8, col = "red", lty = 2)  # Line at 80% variance explained for reference
# Extracting the first 8 principal components
pc_scores <- pca_result$x[, 1:8]
# Creating a new dataframe with PC scores
pc_data <- as.data.frame(pc_scores)
# Adding the target variable back to the dataframe
pc_data$poor_20 <- data$poor_20
saveRDS(pc_data, "../dta_files/pc_data.rds")
data <- readRDS("../dta_files/data_over_sampled.rds")
#----
# A Poor Means Test Replication Study
#Classification using SVM and XGBoost
#Author : Sol Yates
## TO DO - DOUBLE CHECK confusion matrices, implement SVR + CART?
rm(list=ls())
setwd("/home/oddish3/Documents/R_folder/UG/PMT/Scripts")
# Required libraries
library(haven)
library(e1071)
library(magrittr)
library(dplyr)
library(caTools)
library(ggplot2)
library(xgboost)
library(caret)
library(pROC)
library(dplyr)
#library(smotefamily)
library(xtable)
library(rBayesianOptimization)
data <- readRDS("../dta_files/data_over_sampled.rds")
set.seed(122)
# Train/test split 80% train, 20% test
data$poor_20
# Train/test split 80% train, 20% test
set.seed(122)
train_index <- createDataPartition(data$poor_20, p = 0.8, list = FALSE, times = 1)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]
# kernels to iterate through
kernels <- c("linear", "radial", "polynomial", "sigmoid")
# Initialise
metrics_table <- data.frame(Kernel = character(), Accuracy = numeric(), Precision = numeric(),
sensitivity = numeric(), F1_Score = numeric(), AUC = numeric(),
stringsAsFactors = FALSE)
# Iterate through each kernel
for (kernel in kernels) {
formula <- poor_20 ~ toilet_pit + wall_finish +
fuel_elecgas + fuel_charcoal + urban + female_head +
edu_head_primary + edu_head_secondary + div_sep_head + widow_head +
work_paid_head + work_selfemp_nonf_head
# Train the SVM model
svm_model <- svm(formula, data = train_data, kernel = kernel, cost = 1, probability = TRUE)
# Predict on test data
preds_prob <- predict(svm_model, test_data, type = "probabilities")
# Confusion Matrix
cm <- table(test_data$poor_20, preds_prob, dnn = c("Actual", "Predicted"))
# Calculate metrics
accuracy <- sum(diag(cm)) / sum(cm)
precision <- cm[2, 2] / sum(cm[, 2])
sensitivity <- cm[2, 2] / sum(cm[2, ])
f1_score <- 2 * ((precision * sensitivity) / (precision + sensitivity))
roc_obj <- roc(test_data$poor_20, as.numeric(preds_prob))
auc <- auc(roc_obj)
# Add metrics to the table
metrics_table <- rbind(metrics_table, data.frame(Kernel = kernel, Accuracy = accuracy,
Precision = precision, sensitivity = sensitivity,
F1_Score = f1_score, AUC = auc))
}
# Print the metrics table
print(metrics_table)
xtable(cm)
xtable(metrics_table)
xtable(metrics_table)
# Define a smaller hyperparameter grid
hyperparameters <- expand.grid(
C = seq(0.1, 1, by = 0.1),
sigma = c(0.01, 0.1)  # Smaller sigma values
)
# Specify the features formula
features_formula <- formula(paste("poor_20 ~ toilet_pit + wall_finish + fuel_elecgas + fuel_charcoal + urban + female_head + edu_head_primary + edu_head_secondary + div_sep_head + widow_head + work_paid_head + work_selfemp_nonf_head"))
# Perform grid search with cross-validation
ctrl <- trainControl(method = "cv", number = 5)  # 5-fold cross-validation
svm_grid <- train(features_formula, data = train_data, method = "svmRadial", tuneGrid = hyperparameters, trControl = ctrl, preProcess = c("center", "scale"))
# Get the best SVM model
best_svm_model <- svm_grid$finalModel
test_preds <- predict(svm_model, newdata = test_data, probability = TRUE)
test_preds_prob <- attr(test_preds, "probabilities")[, "Class1"]
# Get the best SVM model
best_svm_model <- svm_grid$finalModel
View(best_svm_model)
test_preds <- predict(svm_model, newdata = test_data, probability = TRUE)
test_preds_prob <- attr(test_preds, "probabilities")[, "Class1"]
attr(test_preds, "probabilities")[, "Class1"]
test_preds
test_preds_prob <- attr(test_preds, "probabilities")[, "Class1"]
summary(test_preds)
test_preds
attr(test_preds, "probabilities")
test_preds_prob <- attr(test_preds, "probabilities")[, "1"]
# cm <- table(test_data$poor_20, ifelse(test_preds_prob > 0.5, 1, 0), dnn = c("Actual", "Predicted"))
accuracy <- sum(diag(cm)) / sum(cm)
precision <- cm[2, 2] / sum(cm[, 2])
sensitivity <- cm[2, 2] / sum(cm[2, ])
f1_score <- 2 * ((precision * sensitivity) / (precision + sensitivity))
# Calculate ROC AUC
roc_obj <- roc(test_data$poor_20, test_preds_prob)
auc <- auc(roc_obj)
# Create a data frame to store the metrics
metrics_table <- data.frame(Kernel = "Radial", Cost = svm_model$cost, Gamma = svm_model$gamma,
Accuracy = accuracy, Precision = precision, Sensitivity = sensitivity, F1_Score = f1_score, AUC = auc)
# Print the metrics table
print(metrics_table)
cm
xtable(cm)
xtable(metrics_table)
svmSigmoidModel <- list(
type = "Classification",
library = "e1071",
loop = NULL,
method = "svmSigmoid",
parameters = data.frame(parameter = c('cost', 'gamma'), class = c('numeric', 'numeric')),
grid = function(x, y, len = NULL, search = "grid") {
if(search == "grid") {
expand.grid(cost = 10^(-1:2), gamma = 10^(-2:1))
} else {
data.frame(cost = runif(30, -2, 2), gamma = runif(30, -2, 2))
}
},
fit = function(x, y, wts, param, lev, last, classProbs, ...) {
svm(x = x, y = y, kernel = "sigmoid", cost = param$cost, gamma = param$gamma, probability = TRUE, ...)
},
predict = function(modelFit, newdata, preProc = NULL, submodels = NULL) {
predict(modelFit, newdata)
},
prob = function(modelFit, newdata, preProc = NULL, submodels = NULL) {
preds <- predict(modelFit, newdata, probability = TRUE)
attr(preds, "probabilities")
},
predictors = function(x, ...) {
model.matrix(~ . -1, data = as.data.frame(x))
},
varImp = NULL,
levels = function(x) levels(x$y),
tags = c("SVM", "Kernel", "Sigmoid")
)
# Define the control using a cross-validation approach
train_control <- trainControl(method = "cv", number = 10,
summaryFunction = twoClassSummary,  # Function to use AUC
classProbs = TRUE,  # Probabilities required for AUC
selectionFunction = "best")
# Define the grid for tuning
grid <- expand.grid(cost = 10^(-1:2), gamma = 10^(-2:1))
svm_model <- train(poor_20 ~ ., data = train_data, method = svmSigmoidModel,
trControl = train_control, tuneGrid = grid, preProcess = c("center", "scale"),
metric = "ROC")  # Specify ROC as the metric for model tuning
#----
# A Poor Means Test Replication Study
#Classification using SVM and XGBoost
#Author : Sol Yates
## TO DO - DOUBLE CHECK confusion matrices, implement SVR + CART?
rm(list=ls())
setwd("/home/oddish3/Documents/R_folder/UG/PMT/Scripts")
# Required libraries
library(haven)
library(e1071)
library(magrittr)
library(dplyr)
library(caTools)
library(ggplot2)
library(xgboost)
library(caret)
library(pROC)
library(dplyr)
#library(smotefamily)
library(xtable)
library(rBayesianOptimization)
data <- readRDS("../dta_files/data_ov.rds")
data <- readRDS("../dta_files/data_over_sampled.rds")
set.seed(122)
# Train/test split 80% train, 20% test
data$poor_20 <- factor(data$poor_20, levels = c("0", "1"), labels = c("Class0", "Class1"))
train_index <- createDataPartition(data$poor_20, p = 0.8, list = FALSE, times = 1)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]
View(data)
# kernels to iterate through
kernels <- c("linear", "radial", "polynomial", "sigmoid")
# Initialise
metrics_table <- data.frame(Kernel = character(), Accuracy = numeric(), Precision = numeric(),
sensitivity = numeric(), F1_Score = numeric(), AUC = numeric(),
stringsAsFactors = FALSE)
# Iterate through each kernel
for (kernel in kernels) {
formula <- poor_20 ~ toilet_pit + wall_finish +
fuel_elecgas + fuel_charcoal + urban + female_head +
edu_head_primary + edu_head_secondary + div_sep_head + widow_head +
work_paid_head + work_selfemp_nonf_head
# Train the SVM model
svm_model <- svm(formula, data = train_data, kernel = kernel, cost = 1, probability = TRUE)
# Predict on test data
preds_prob <- predict(svm_model, test_data, type = "probabilities")
# Confusion Matrix
cm <- table(test_data$poor_20, preds_prob, dnn = c("Actual", "Predicted"))
# Calculate metrics
accuracy <- sum(diag(cm)) / sum(cm)
precision <- cm[2, 2] / sum(cm[, 2])
sensitivity <- cm[2, 2] / sum(cm[2, ])
f1_score <- 2 * ((precision * sensitivity) / (precision + sensitivity))
roc_obj <- roc(test_data$poor_20, as.numeric(preds_prob))
auc <- auc(roc_obj)
# Add metrics to the table
metrics_table <- rbind(metrics_table, data.frame(Kernel = kernel, Accuracy = accuracy,
Precision = precision, sensitivity = sensitivity,
F1_Score = f1_score, AUC = auc))
}
# Print the metrics table
print(metrics_table)
xtable(cm)
# Print the metrics table
print(metrics_table)
xtable(cm)
xtable(metrics_table)
# Define a smaller hyperparameter grid
hyperparameters <- expand.grid(
C = seq(0.1, 1, by = 0.1),
sigma = c(0.01, 0.1)  # Smaller sigma values
)
# Specify the features formula
features_formula <- formula(paste("poor_20 ~ toilet_pit + wall_finish + fuel_elecgas + fuel_charcoal + urban + female_head + edu_head_primary + edu_head_secondary + div_sep_head + widow_head + work_paid_head + work_selfemp_nonf_head"))
# Perform grid search with cross-validation
ctrl <- trainControl(method = "cv", number = 5)  # 5-fold cross-validation
svm_grid <- train(features_formula, data = train_data, method = "svmRadial", tuneGrid = hyperparameters, trControl = ctrl, preProcess = c("center", "scale"))
# Get the best SVM model
best_svm_model <- svm_grid$finalModel
test_preds <- predict(svm_model, newdata = test_data, probability = TRUE)
test_preds_prob <- attr(test_preds, "probabilities")[, "Class1"]
cm <- table(test_data$poor_20, ifelse(test_preds_prob > 0.5, 1, 0), dnn = c("Actual", "Predicted"))
accuracy <- sum(diag(cm)) / sum(cm)
precision <- cm[2, 2] / sum(cm[, 2])
sensitivity <- cm[2, 2] / sum(cm[2, ])
f1_score <- 2 * ((precision * sensitivity) / (precision + sensitivity))
# Calculate ROC AUC
roc_obj <- roc(test_data$poor_20, test_preds_prob)
auc <- auc(roc_obj)
# Create a data frame to store the metrics
metrics_table <- data.frame(Kernel = "Radial", Cost = svm_model$cost, Gamma = svm_model$gamma,
Accuracy = accuracy, Precision = precision, Sensitivity = sensitivity, F1_Score = f1_score, AUC = auc)
# Print the metrics table
print(metrics_table)
xtable(cm)
xtable(metrics_table)
svmSigmoidModel <- list(
type = "Classification",
library = "e1071",
loop = NULL,
method = "svmSigmoid",
parameters = data.frame(parameter = c('cost', 'gamma'), class = c('numeric', 'numeric')),
grid = function(x, y, len = NULL, search = "grid") {
if(search == "grid") {
expand.grid(cost = 10^(-1:2), gamma = 10^(-2:1))
} else {
data.frame(cost = runif(30, -2, 2), gamma = runif(30, -2, 2))
}
},
fit = function(x, y, wts, param, lev, last, classProbs, ...) {
svm(x = x, y = y, kernel = "sigmoid", cost = param$cost, gamma = param$gamma, probability = TRUE, ...)
},
predict = function(modelFit, newdata, preProc = NULL, submodels = NULL) {
predict(modelFit, newdata)
},
prob = function(modelFit, newdata, preProc = NULL, submodels = NULL) {
preds <- predict(modelFit, newdata, probability = TRUE)
attr(preds, "probabilities")
},
predictors = function(x, ...) {
model.matrix(~ . -1, data = as.data.frame(x))
},
varImp = NULL,
levels = function(x) levels(x$y),
tags = c("SVM", "Kernel", "Sigmoid")
)
# Define the control using a cross-validation approach
train_control <- trainControl(method = "cv", number = 10,
summaryFunction = twoClassSummary,  # Function to use AUC
classProbs = TRUE,  # Probabilities required for AUC
selectionFunction = "best")
# Define the grid for tuning
grid <- expand.grid(cost = 10^(-1:2), gamma = 10^(-2:1))
svm_model <- train(poor_20 ~ ., data = train_data, method = svmSigmoidModel,
trControl = train_control, tuneGrid = grid, preProcess = c("center", "scale"),
metric = "ROC")  # Specify ROC as the metric for model tuning
